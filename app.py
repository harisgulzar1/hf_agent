import os
from dotenv import load_dotenv
import requests
import pandas as pd
import inspect
from IPython.display import Image, display
import mimetypes
import base64
import tempfile
from langchain_core.tools import tool
from typing import TypedDict, Annotated
from langgraph.graph.message import add_messages
from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage
from langgraph.prebuilt import ToolNode
from langgraph.graph import START, StateGraph, END
from langgraph.prebuilt import tools_condition
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from openai import OpenAI
from langsmith import traceable
from langsmith.wrappers import wrap_openai
from langchain.callbacks.tracers import LangChainTracer
import os
import base64


# Load environment variables from .env file
load_dotenv()

# Optionally set the project name programmatically
os.environ["LANGCHAIN_PROJECT"] = os.getenv("LANGCHAIN_PROJECT", "MultiInputAgentTrace")

# Get values
HF_USERNAME = os.getenv("HF_USERNAME")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
API_URL = os.getenv("API_URL", "https://jofthomas-unit4-scoring.hf.space/")

# OpenAI native client for tool use
openai_client = OpenAI(api_key=OPENAI_API_KEY)

@tool
def openai_web_search(query: str) -> str:
    """
    Perform a web search using OpenAI's web_search_preview tool.

    Args:
        query (str): The search query string.

    Returns:
        str: A summarized result from the web search.
    """
    print("WEB TOOL EXECUTED")
    response = openai_client.responses.create(
        model="gpt-4.1",
        tools=[{"type": "web_search_preview"}],
        input=query
    )
    return response.output_text

@tool
def image_analyzer_tool(image_path: str, prompt: str = "Describe this image.") -> str:
    """
    Analyze an image file and return a detailed description using OpenAI's vision model.

    Args:
        image_path (str): Path to the local image file.
        prompt (str, optional): Natural language prompt to guide the analysis. Defaults to "Describe this image."

    Returns:
        str: The description generated by the vision model.
    """
    print("IMAGE TOOL EXECUTED")
    if not os.path.isfile(image_path):
        return "Error: Provided image path does not exist."
    import mimetypes
    mime_type, _ = mimetypes.guess_type(image_path)
    mime_type = mime_type or "image/png"
    with open(image_path, "rb") as img_file:
        image_bytes = img_file.read()
    base64_img = base64.b64encode(image_bytes).decode("utf-8")
    response = openai_client.chat.completions.create(
        model="gpt-4-vision-preview",
        messages=[
            {"role": "system", "content": "You are a helpful assistant who analyzes images."},
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": f"data:{mime_type};base64,{base64_img}"}}
                ]
            }
        ],
        max_tokens=1000
    )
    try:
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"Error analyzing image: {e}"

@tool
def audio_transcription_tool(audio_path: str, prompt: str = "Transcribe this audio.") -> str:
    """
    Transcribe an audio file to text using OpenAI's Whisper model.

    Args:
        audio_path (str): Path to the local audio file.
        prompt (str, optional): Prompt for the transcription model. Defaults to "Transcribe this audio."

    Returns:
        str: The transcription as plain text.
    """
    print("AUDIO TOOL EXECUTED")
    with open(audio_path, "rb") as audio_file:
        transcript = openai_client.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file,
            response_format="text",
            prompt=prompt
        )
    return transcript.strip()

import re

@tool
def video_analysis_tool(youtube_url: str, prompt: str = "Transcribe or summarize the video.") -> str:
    """
    Analyze a YouTube video: download, transcribe, and return summary or transcript.

    Args:
        youtube_url (str): A direct link to a YouTube video.
        prompt (str, optional): Additional instructions (e.g., answer a question about the video). Defaults to generic.

    Returns:
        str: Transcript or answer based on the prompt.
    """
    print("VIDEO TOOL EXECUTED")
    # Placeholder for actual logic. Integrate with your video/audio backend, or use yt-dlp + Whisper, or a paid API.
    # For demo: just return the URL and prompt.
    return f"[Simulated video analysis] For video: {youtube_url} | Prompt: {prompt}"


tools = [openai_web_search, image_analyzer_tool, audio_transcription_tool, video_analysis_tool]

print("Registered tools:", [t.name for t in tools])


llm = ChatOpenAI(
    model="gpt-4.1-2025-04-14",
    api_key=OPENAI_API_KEY,
    temperature=0.7,
    verbose=True,
    callbacks=[LangChainTracer()]
).bind_tools(tools)

class AgentState(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]
    input_file: str #Annotated[str, add_messages]
    iteration: int


def extract_final_answer(content: str) -> str:
    if "FINAL ANSWER:" in content:
        return content.split("FINAL ANSWER:", 1)[1].strip()
    return content.strip()

# --- Handler node ---
def get_valid_file_path(input_file):
    if isinstance(input_file, list):
        # Find the last item that is a non-empty string and a valid file
        for v in reversed(input_file):
            if isinstance(v, str) and v and os.path.isfile(v):
                return v
        return None
    if isinstance(input_file, str) and input_file and os.path.isfile(input_file):
        return input_file
    return None

# @traceable(name="handler-node")
# def handler_node(state: AgentState):
#     """
#     Decides what tools are needed (web, image, audio, video), encourages tool use,
#     and does NOT produce final answers. When ready, says 'READY FOR FINAL ANSWER'.
#     """
#     import json
#     messages = state["messages"]
#     input_file = state.get("input_file")
#     iteration = state.get("iteration", 0)

#     input_file_val = get_valid_file_path(input_file)
#     tool_calls = []
#     # 1. File-based tool
#     if input_file_val and os.path.isfile(input_file_val):
#         mime_type, _ = mimetypes.guess_type(input_file_val)
#         if mime_type:
#             if mime_type.startswith("image"):
#                 tool_calls.append({"tool": "image_analyzer_tool", "args": {"image_path": input_file_val, "prompt": "Describe this image in detail."}})
#             elif mime_type.startswith("audio"):
#                 tool_calls.append({"tool": "audio_transcription_tool", "args": {"audio_path": input_file_val, "prompt": "Transcribe this audio as plain text."}})
#     # 2. YouTube tool
#     youtube_regex = r"(https?://(?:www\.)?(?:youtube\.com|youtu\.be)/[\w\-?&=;#./]+)"
#     user_query = ""
#     for m in reversed(messages):
#         if isinstance(m, HumanMessage):
#             user_query = m.content
#             break
#     youtube_links = re.findall(youtube_regex, user_query)
#     if youtube_links:
#         yt_url = youtube_links[0]
#         tool_calls.append({
#             "tool": "video_analysis_tool",
#             "args": {
#                 "youtube_url": yt_url,
#                 "prompt": user_query
#             }
#         })
#     # If any tool should be called, output a function call
#     if tool_calls:
#         tool = tool_calls[0]
#         tool_name = tool["tool"]
#         tool_args = json.dumps(tool["args"])
#         print(f"[DEBUG handler_node] Emitting tool call: {tool_name} with args: {tool_args}")

#         return {
#             "messages": [
#                 AIMessage(
#                     content="",
#                     additional_kwargs={
#                         "function_call": {
#                             "name": tool_name,
#                             "arguments": tool_args
#                         }
#                     }
#                 )
#             ],
#             "input_file": input_file,
#             "iteration": iteration + 1
#         }

#     # Otherwise, step-by-step reasoning, but NO FINAL ANSWER
#     tool_descriptions = '''
#             openai_web_search(query: str) -> str:
#                 Use for up-to-date or external info from the web.

#             image_analyzer_tool(image_path: str, prompt: str = 'Describe this image.') -> str:
#                 Analyze images for content and details.

#             audio_transcription_tool(audio_path: str, prompt: str = 'Transcribe this audio.') -> str:
#                 Transcribe audio to text.

#             video_analysis_tool(youtube_url: str, prompt: str = 'Transcribe or summarize the video.') -> str:
#                 Analyze a YouTube video for transcript or summary.
#             '''
#     file_context = input_file_val or "None"
#     sys_msg = SystemMessage(
#         content=f"""
#             You are an AI assistant that thinks step by step and is encouraged to call tools for external information, files, or videos. 
#             Never produce the final answer directly in this step. If you believe you have ALL the information needed, respond with ONLY:
#             READY FOR FINAL ANSWER

#             Current input file (if any): {file_context}

#             If you need to use a tool, describe why and call the tool. Otherwise, if all information is present, say READY FOR FINAL ANSWER.

#             Available tools:
#             {tool_descriptions}
#             """)
#     chat_messages = [sys_msg] + [
#         {"role": "user" if isinstance(m, HumanMessage) else "assistant", "content": m.content}
#         for m in state["messages"]
#     ]
#     response = llm.invoke(chat_messages)
#     # If the LLM says 'READY FOR FINAL ANSWER' (case-insensitive), transition to output_node
#     if "ready for final answer" in response.content.lower():
#         return {
#             "messages": [AIMessage(content="READY FOR FINAL ANSWER")],
#             "input_file": file_context,
#             "iteration": iteration
#         }
#     # If max iterations reached, force finalize
#     if state.get("iteration") >= 3:
#         print("ITERATION CONDITION MET")
#         return {
#             "messages": [AIMessage(content="READY FOR FINAL ANSWER")],
#             "input_file": file_context,
#             "iteration": iteration
#         }
#     # Otherwise, keep looping
#     return {
#         "messages": [AIMessage(content=response.content)],
#         "input_file": file_context,
#         "iteration": iteration
#     }

# --- Output node ---
@traceable(name="output-node")
def output_node(state: AgentState):
    input_file = state.get("input_file")
    input_file_val = get_valid_file_path(input_file)
    """
    Now that all information and tool results are available, produce the final answer in the required format.
    """
    file_context = input_file_val or "None"
    tool_descriptions = '''
            openai_web_search(query: str) -> str: For web info.
            image_analyzer_tool(image_path: str, prompt: str) -> str: For image analysis.
            audio_transcription_tool(audio_path: str, prompt: str) -> str: For audio to text.
            video_analysis_tool(youtube_url: str, prompt: str) -> str: For YouTube video transcript/summary.
            '''
    sys_msg = SystemMessage(
        content=f"""
            You are a general AI assistant. Now use all the provided information, tool outputs, and user question to answer in this format:
            FINAL ANSWER: [YOUR FINAL ANSWER]
            Your answer should be a number OR as few words as possible OR a comma-separated list (according to instructions).
            Do NOT call tools. Just answer.

            If there is an input file, it has already been processed:
            Attached input file (if any): {file_context}

            User's question and tool results are above. 
            Remember: Only output FINAL ANSWER: ... in this step!
            """
    )
    messages = state["messages"]
    chat_messages = [sys_msg] + [
        {"role": "user" if isinstance(m, HumanMessage) else "assistant", "content": m.content}
        for m in messages
    ]
    response = llm.invoke(chat_messages)
    final_answer = extract_final_answer(response.content)
    return {
        "messages": [AIMessage(content=f"FINAL ANSWER: {final_answer}")],
        "input_file": file_context,
        "iteration": state.get("iteration")
    }


# Handler node: If LLM produces a tool call (function_call), go to tool; if "READY FOR FINAL ANSWER", go to output; else loop

# def handler_tools_condition(state):
#     msg = state["messages"][-1]
#     print("=== handler_tools_condition ===")
#     print("msg:", msg)
#     if state.get("iteration") >= 3:
#         print("Iteration limit reached, going to output.")
#         return "output"
#     if hasattr(msg, "additional_kwargs") and "function_call" in msg.additional_kwargs:
#         print("Tool call detected.")
#         return "tools"
#     if hasattr(msg, "content") and "ready for final answer" in msg.content.lower():
#         print("Detected READY FOR FINAL ANSWER, going to output.")
#         return "output"
#     print("No tool call or finalize, defaulting to output (force finalize).")
#     return "output"


# ------------------ Main loop (refactored) ------------------
@traceable(name="condition")
def should_continue(state: AgentState):
    """Route to tools if the LLM's last message includes tool calls; otherwise finish."""
    messages = state["messages"]
    iteration = state.get("iteration", 0)
    last = messages[-1]
    if getattr(last, "tool_calls", None) and iteration < 3:
        return "tools"
    return "output"

@traceable(name="handler")
def handler_node(state: AgentState) -> AgentState:
    messages = state["messages"]
    iteration = state.get("iteration", 0)

    # Encourage tool use via a light system prompt, but DO NOT craft function_call payloads yourself.
    sys = SystemMessage(content=(
        "You are a helpful assistant. Use tools when needed. "
        "If you have enough information to answer, answer concisely."
    ))

    response = llm.invoke([sys, *messages])
    return {
        "messages": messages + [response],
        "input_file": state.get("input_file", ""),
        "iteration": iteration + 1,
    }


tool_node = ToolNode(tools=tools)

# Define the graph
builder = StateGraph(AgentState)
builder.add_node("handler", handler_node)
builder.add_node("output", output_node)
builder.add_node("tools", tool_node)

builder.add_edge(START, "handler")
builder.add_conditional_edges("handler", should_continue, ["output", "tools"])
builder.add_edge("tools", "handler")
builder.add_edge("output", END)

alfred = builder.compile()

with open("graph_diagram.png", "wb") as f:
    f.write(alfred.get_graph().draw_mermaid_png())

@traceable(name="run_and_submit_all_agent")
def run_and_submit_all():
    if not HF_USERNAME:
        print("HF_USERNAME is not set in the environment.")
        return

    questions_url = f"{API_URL}/questions"
    submit_url = f"{API_URL}/submit"

    agent_code = inspect.getsource(AgentState)

    try:
        response = requests.get(questions_url, timeout=10)
        response.raise_for_status()
        questions = response.json()
    except Exception as e:
        print("Error fetching questions:", e)
        return

    answers = []
    for q in questions:
        input_content = q.get("question", "")
        file_context = ""
        task_id = q.get("task_id")
        file_data = q.get("file_name")
        if isinstance(file_data, str) and file_data:
            try:
                # Construct URL to download file (assuming the API supports it)
                file_url = f"{API_URL}/files/{task_id}"
                file_response = requests.get(file_url)
                file_response.raise_for_status()
                mime_type, _ = mimetypes.guess_type(file_data)
                suffix = mimetypes.guess_extension(mime_type) or os.path.splitext(file_data)[1] or ""
                with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
                    tmp.write(file_response.content)
                    file_context = tmp.name
            except Exception as fe:
                print("Exception is executed.")
                input_content += f"\n\nError loading attached file: {fe}"
        raw_answer = alfred.invoke({
            "messages": [HumanMessage(content=input_content)],
            "input_file": file_context,
            "iteration": 0
        })['messages'][-1].content
        # Remove leading "FINAL ANSWER:" if present, and strip whitespace
        if raw_answer.strip().startswith("FINAL ANSWER:"):
            answer = raw_answer.strip()[len("FINAL ANSWER:"):].strip()
        else:
            answer = raw_answer.strip()
        print("Question:" + q.get("question", "") + "\n" + "Answer:" + answer)
        answers.append({
            "task_id": q["task_id"],
            "submitted_answer": answer,
        })
    submission = {
        "username": HF_USERNAME,
        "agent_code": agent_code,
        "answers": answers
    }
    try:
        res = requests.post(submit_url, json=submission, timeout=15)
        res.raise_for_status()
        result = res.json()
        print("Submission Result:", result)
    except Exception as e:
        print("Submission failed:", e)

import langsmith

if __name__ == "__main__":
    # Enable fine-grained tracing for LangGraph and LangSmith
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_PROJECT"] = os.getenv("LANGCHAIN_PROJECT", "MultiInputAgentTrace")
    run_and_submit_all()
